{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ada82c-b519-4af0-817c-a7e0bcfbe68d",
   "metadata": {},
   "source": [
    "# **NLP com Flair**\n",
    "\n",
    "# 3' - Treinando Transformer com Flair\n",
    "\n",
    "\n",
    "**PROBLEMA**: Não parece funcionar com o tokenizer do transformer.\n",
    "\n",
    "Como já apresentamos até agora, o pacote Flair possui funções que ajudam bastante na hora de\n",
    "treinar um modelo. Porém, se não quisermos usar os modelos default, precisamos\n",
    "fazer as coisas um pouco diferente.\n",
    "\n",
    "Nessa seção, vamos explorar um pouco mais sobre como treinar um modelo com Flair,\n",
    "porém, usando nosso próprio modelo Transformer. Note que nesse caso,\n",
    "queremos usar nosso próprio tokenizador, e nosso próprio embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2fa716-d2a8-4f0e-9276-ac1b8efb6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 21:42:11,774 Reading data from /home/davibarreira/MEGA/resolvvi/lener-br/leNER-Br\n",
      "2022-10-19 21:42:11,776 Train: /home/davibarreira/MEGA/resolvvi/lener-br/leNER-Br/train/train.conll\n",
      "2022-10-19 21:42:11,777 Dev: /home/davibarreira/MEGA/resolvvi/lener-br/leNER-Br/dev/dev.conll\n",
      "2022-10-19 21:42:11,778 Test: /home/davibarreira/MEGA/resolvvi/lener-br/leNER-Br/test/test.conll\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "\n",
    "import random\n",
    "random.seed(123)  # ensure we get same split every time\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/home/davibarreira/MEGA/resolvvi/lener-br/leNER-Br/'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train/train.conll',\n",
    "                              test_file='test/test.conll',\n",
    "                              dev_file='dev/dev.conll')\n",
    "corpus.downsample(0.01)\n",
    "\n",
    "import random\n",
    "random.seed(123)  # ensure we get same split every time\n",
    "\n",
    "\n",
    "transformer = TransformerWordEmbeddings('./models/BERTikal/', layers='-1', layer_mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1e72f9-7a12-4f8f-a810-96172fd6aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/BERTikal/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e61111-8d59-4f5e-84fb-d3fbf15e4519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1416, 125, 374, 8110, 2446, 304, 22280, 3414, 5741, 424, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Exemplo de tokenização usando transformers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10966b29-d1ab-499b-8fc3-4794fe4bd689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
