{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afc1f7a-f485-4066-85cd-c7c5df489809",
   "metadata": {},
   "source": [
    "# **NLP com Flair**\n",
    "Este tutorial é baseado no livro \"Natural Language Processing with Flair\".\n",
    "\n",
    "# 1 - Sentenças, Tokens e Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01241f7a-dcbb-4804-9274-ad02562ff962",
   "metadata": {},
   "source": [
    "## Sentenças"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48ae34-31bc-4bb8-bbfc-e79247ba654c",
   "metadata": {},
   "source": [
    "Primeiro vamos explorar um dos objetos mais fundamentais do Flair, a `Sentence`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3392f70e-2005-4eee-818d-98be79ac558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davibarreira/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Um exemplo de sentença .\" 5\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "sentence = Sentence('Um exemplo de sentença.')\n",
    "print(sentence, len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18d7a7-9a2a-4101-b093-82ba3a29e3f4",
   "metadata": {},
   "source": [
    "Note que a nossa sentença \"Um exemplo de sentença\", o Flair identificou 5 tokens.\n",
    "Uma token é uma \"unidade de texto\". Uma unidade de texto comum de se usar\n",
    "é \"palavra\". Ou seja, uma sentença é composta de palavras individuais. Porém, além de palavras, podemos ter coisas como\n",
    "pontuação ou nome próprios. Por exemplo, \"João Miguel foi à praia.\".  Podemos identificar \"João Miguel\" como uma token só.\n",
    "Isso faz mais sentido na hora de analisar esse texto, pois \"João Miguel\" é na verdade o nome de uma só pessoa, e não dois indivíduos.\n",
    "De forma similar, na sentença \"A Universidade Federal do Ceará é uma boa universidade\", temos a token \"Universidade Federal do Ceará\"\n",
    "que simboliza uma só entidade.\n",
    "\n",
    "A maneira de identificar essas \"unidades de texto\", ou, tokens, varia. Você pode querer simplesmente quebrar por palavras, ou usar\n",
    "algum outro método. Os modelos que fazem esse tipo de trabalho são os chamado tokenizadores (tokenizer).\n",
    "\n",
    "No Flair, quando não passamos um tokenizador, ele por default utiliza um modelo próprio chamado `SegTokTokenizer`.\n",
    "Esse modelo tokeniza utilizando alguns critérios razoáveis, como pontuação, e espaços em branco.\n",
    "\n",
    "Vamos agora trocar o tokenizador pra mostrar como isso impacta na identificação das nossas tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd42d89-b4bd-46c8-9781-87c66fe0866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Um exemplo de sentença.\" 4\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.tokenization import SpaceTokenizer\n",
    "from flair.tokenization import SpacyTokenizer\n",
    "tokenizer = SpaceTokenizer()\n",
    "s = Sentence('Um exemplo de sentença.', use_tokenizer=tokenizer)\n",
    "# getting the string representation using magic method __str__ \n",
    "print(s, len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1bcb2-797e-47c1-96df-424513d74395",
   "metadata": {},
   "source": [
    "Agora, invés de 5, obtivemos 4, pois no modelo `SpaceTokenizer`, somente espaços \" \" são usados pra tokenizar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2f705-763d-4e45-9ead-d8981707349b",
   "metadata": {},
   "source": [
    "O objeto `Sentence` é composto de tokens, que podem ser acessada facilmente.\n",
    "Ele pode ser acessado como uma lista, ou usando o método `get_token`. Note\n",
    "que o `get_token` começa a indexação por 1 invés de 0 (padrão de python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085caa24-32fb-46f9-8797-5e03f2870c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Um exemplo de sentença.\" → [\"Um\"/DT]\n",
      "Token[0]: \"Um\" → DT (1.0)\n",
      "Token[0]: \"Um\" → DT (1.0)\n",
      "Token[1]: \"exemplo\"\n",
      "Token[2]: \"de\"\n",
      "Token[3]: \"sentença.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get_token(1).add_label('manual-pos', 'DT')\n",
    "print(s)\n",
    "\n",
    "\n",
    "print(s[0])\n",
    "for token in s:\n",
    "    print(token)\n",
    "    \n",
    "len(s), len(s.to_original_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94e1e91-cd40-4e1f-ad68-cf1683958924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.tokenization import SpacyTokenizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5677bb0-2e80-4ec5-81e5-dd424653986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Moro na cidade de São Paulo .\" 7\n",
      "Token[0]: \"Moro\"\n",
      "Token[1]: \"na\"\n",
      "Token[2]: \"cidade\"\n",
      "Token[3]: \"de\"\n",
      "Token[4]: \"São\"\n",
      "Token[5]: \"Paulo\"\n",
      "Token[6]: \".\"\n"
     ]
    }
   ],
   "source": [
    "model = spacy.load(\"pt_core_news_lg\")\n",
    "tokenizer = SpacyTokenizer(model)\n",
    "\n",
    "s = Sentence('Moro na cidade de São Paulo.', use_tokenizer=tokenizer)\n",
    "print(s, len(s))\n",
    "for token in s:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8f825-3fc8-48ae-a093-e7d7ead460e1",
   "metadata": {},
   "source": [
    "## Tokenizer Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae68dea0-7997-4eb3-b9cd-56459f0fa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Token\n",
    "from flair.tokenization import TokenizerWrapper\n",
    "\n",
    "def char_splitter(string):\n",
    "    return list(string)\n",
    "char_tokenizer = TokenizerWrapper(char_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8193d6-ae3f-4f32-8693-acddc7ce1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"T\"\n",
      "Token[1]: \"e\"\n",
      "Token[2]: \"s\"\n",
      "Token[3]: \"t\"\n",
      "Token[4]: \"e\"\n",
      "Token[5]: \" \"\n",
      "Token[6]: \"t\"\n",
      "Token[7]: \"e\"\n",
      "Token[8]: \"x\"\n",
      "Token[9]: \"t\"\n",
      "Token[10]: \"o\"\n",
      "Token[11]: \".\"\n"
     ]
    }
   ],
   "source": [
    "text = \"Teste texto.\"\n",
    "sentence = Sentence(text, use_tokenizer=char_tokenizer)\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce7bc4-0bef-46f0-9f24-38e8839cfd4b",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Um corpus é uma coleção de textos. No Flair existem alguns corpus padrões que podemos importar.\n",
    "Esses corpus já vem dividos em train, test e dev. Nesse caso, o dev é o que normalmente chamamos de validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc11565-b35d-4a32-a2f9-fa11f49b1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 10:57:19,566 Reading data from /home/davibarreira/.flair/datasets/ud_portuguese\n",
      "2022-10-18 10:57:19,570 Train: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-train.conllu\n",
      "2022-10-18 10:57:19,571 Dev: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-dev.conllu\n",
      "2022-10-18 10:57:19,573 Test: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-test.conllu\n",
      "Corpus: 7018 train + 1172 dev + 1167 test sentences\n"
     ]
    }
   ],
   "source": [
    "from flair import datasets\n",
    "\n",
    "corpus = datasets.UD_PORTUGUESE()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "935eac78-c2ae-4594-8c72-2e4ccda2fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = corpus.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304e876-21aa-4843-89b7-88aeaa31393d",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Vamos recapitular a estrutura hierárquica que temos em NLP. A maior unidade é o corpus, que é uma coleção de textos. A segunda unidade é uma \"sentença\". Aqui,\n",
    "uma sentença não significa uma frase, mas simplesmente o texto que estamos focando pra realizar algum tipo de análise.\n",
    "Por exemplo, podemos ter um corpus de reviews de filmes. Cada review seria tratada como uma \"sentença\", que compõe o corpus.\n",
    "A terceira unidade é a token, que, como já falamos, é análogo à ideia de palavras.\n",
    "\n",
    "Com isso definido, o passo seguinte é entender como representar essas tokens no computador. A ideia é que a token (e.g. \"carro\")\n",
    "deve ser representada de forma que possamos manipulá-la de forma inteligente. Por exemplo, seria interessante que de alguma\n",
    "forma conseguissemos dizer que a token \"carro\" é parecida/próxima da token \"veículo\". A representação de uma token é o que chamamos\n",
    "de embedding. No geral, a ideia envolve transformar um texto é um objeto matemático com propriedades interessantes (e.g. um vetor).\n",
    "\n",
    "Dito isso, existem vários métodos diferentes para fazer um embedding. Vamos começar com alguns dos clássicos.\n",
    "A lista de modelos disponibilizados pelo Flair pode ser achados [aqui](https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/CLASSIC_WORD_EMBEDDINGS.md).\n",
    "\n",
    "Um tipo comum de embedding é o one-hot-encoding, onde simplesmente pegamos um texto e fazemos cada coluna como uma palavra, sendo 0 ou 1 caso\n",
    "aquela token seja essa palavra específica.\n",
    "O problema desse tipo de representação é se tivermos uma palavra nova, ela não será contemplada. Além disso, teremos colunas demais, e não será\n",
    "possível fazer o tipo de manipulação que comentamos, onde queremos que \"carro\" se pareça com \"veículo\".\n",
    "\n",
    "Para possibilitar esse tipo de \"matemática das tokens\", precisamos de um modelo mais complexo, e que de alguma forma tenha sido\n",
    "treinado em textos daquela língua (e.g. português), de forma a entender que \"carro\" e \"veículo\" ocorrem em contextos parecidos,\n",
    "e, portanto, são palavras parecidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2201b785-f868-4ff3-80cd-940209e497f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "fasttext = WordEmbeddings('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ddc90e-285b-485c-9994-2ad9d474059d",
   "metadata": {},
   "source": [
    "O código acima baixou o modelo de embedding chamado \"FastText\" treinado em textos em português. Se você está usando Linux,\n",
    "o Flair criou uma pasta `/home/./flair` onde salvou os embeddings. Assim, ele não precisa baixar novamente toda vida que for usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa0b4ee-5292-448d-8191-a628a5096dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"Meu carro é azul . Ele é o veículo que uso para viajar .\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenca = Sentence(\"Meu carro é azul. Ele é o veículo que uso para viajar.\")\n",
    "fasttext.embed(sentenca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa54173-c5fa-4052-b22e-aa1dff4a6147",
   "metadata": {},
   "source": [
    "O que aconteceu? O nosso modelo `fasttext` criou uma representação vetorial de cada token da nossa senteça. \n",
    "Essas representações ficam gravadas dentro do objeto do tipo `Sentence`, ou seja, dentro da variável `sentenca`.\n",
    "Para acessar a representação vetorial, basta usar `sentenca[0].embedding`. Isso nos retornará\n",
    "um objeto do tipo torch.Tensor do pacote Pytorch... Para transformar em um \"vetor\", podemos usar\n",
    "`sentenca[0].embedding.tolist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb48c5fc-dc96-4c30-a1aa-64e472822924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[1]: \"carro\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2702e-02, -1.9549e-01,  2.9307e-02, -1.3602e-01,  1.2380e-01,\n",
       "         5.3440e-03,  2.0641e-01, -4.3588e-02, -1.9815e-01,  2.0293e-01,\n",
       "         4.1532e-02,  4.1676e-01,  1.3775e-01,  4.7727e-01,  1.0198e-01,\n",
       "        -1.3813e-01,  4.0064e-02, -7.5214e-02, -1.5032e-01, -6.4728e-03,\n",
       "        -1.7995e-01, -5.2995e-01,  2.0694e-01, -3.2790e-01,  2.9773e-01,\n",
       "        -1.0036e-02, -4.4665e-01, -3.4481e-01,  3.7361e-01, -2.5838e-01,\n",
       "         4.4260e-02, -7.0744e-02, -1.5724e-01,  2.6081e-01, -3.1650e-01,\n",
       "        -5.1831e-01,  3.2216e-01,  1.1580e-01, -2.2487e-01, -4.5416e-02,\n",
       "         1.9262e-01,  4.2113e-01,  1.1376e-02,  2.2154e-01,  7.5462e-02,\n",
       "         7.8880e-02,  1.3207e-01,  2.1083e-01, -1.7563e-01,  8.2525e-02,\n",
       "        -2.7856e-02, -4.1808e-02, -1.6477e-01, -2.1031e-01, -2.3907e-01,\n",
       "        -1.4270e-01, -1.2981e-01,  2.0888e-01, -3.4326e-02, -3.0938e-01,\n",
       "         5.1215e-02,  3.1539e-01,  3.8521e-02, -1.4974e-01, -6.1015e-02,\n",
       "        -3.9620e-01, -4.0661e-01, -3.0586e-01, -9.3094e-03, -2.2762e-01,\n",
       "         1.8633e-02,  5.1221e-01, -2.6176e-01, -1.9179e-01, -1.8081e-01,\n",
       "        -4.5814e-01, -7.2806e-02,  5.5304e-02, -2.1155e-01,  1.0709e-01,\n",
       "         1.1854e-01, -1.9346e-01,  9.0553e-02, -2.6388e-01, -1.3908e-01,\n",
       "         1.4465e-01,  3.6313e-01,  1.6862e-01,  1.4288e-01, -2.0240e-01,\n",
       "         1.0916e-01, -4.0417e-02, -1.7896e-02, -1.3739e-01,  8.9830e-02,\n",
       "        -3.1596e-02, -3.0207e-01, -1.8777e-02,  1.1466e-03,  2.2828e-01,\n",
       "        -3.5517e-01, -1.9641e-01, -2.2662e-02, -1.5253e-01,  1.6987e-01,\n",
       "        -8.7014e-02, -1.8741e-02, -2.4451e-01,  1.6020e-02, -2.8253e-01,\n",
       "        -2.8261e-02,  4.3644e-01,  5.4464e-04,  7.3343e-02, -4.0997e-01,\n",
       "        -2.3451e-01, -8.1023e-02, -2.1730e-01, -1.8678e-01,  4.1523e-01,\n",
       "         2.6212e-01, -3.7893e-01,  5.6197e-01,  7.1982e-02, -4.5799e-01,\n",
       "        -1.8885e-01, -1.4586e-01,  1.5735e-01, -2.4076e-01, -1.3970e-01,\n",
       "        -2.5083e-01, -9.6650e-02,  3.7597e-01, -1.5726e-01,  8.2422e-02,\n",
       "         2.4621e-02,  6.0302e-02,  3.9360e-01, -1.3873e-01,  1.9037e-01,\n",
       "         2.3333e-01, -2.3979e-01,  1.0205e-01, -1.2616e-01,  1.3697e-01,\n",
       "         2.8724e-01, -9.1617e-02, -1.0724e-01, -5.7052e-01, -9.8063e-02,\n",
       "        -6.5116e-02, -4.2585e-02, -1.4535e-01,  2.4580e-01, -2.5547e-03,\n",
       "        -1.5183e-01,  4.7485e-01, -2.5100e-01,  2.3432e-02, -1.1147e-01,\n",
       "         1.4243e-02,  3.1933e-01, -6.6922e-02,  3.5181e-01,  1.5499e-01,\n",
       "        -1.8776e-01,  1.0422e-01, -3.0188e-01,  1.4442e-01,  9.1441e-02,\n",
       "        -1.5347e-01, -3.1051e-01, -2.0869e-01,  2.8112e-01, -1.2471e-01,\n",
       "         5.2458e-02,  8.2239e-02, -1.2045e-01, -3.0983e-01, -2.3250e-02,\n",
       "         3.0972e-01, -7.5411e-02, -8.8738e-02,  1.5910e-01,  5.3525e-02,\n",
       "        -2.3694e-01, -2.0620e-01, -1.6721e-01,  5.8825e-02, -3.4073e-02,\n",
       "         2.2780e-01,  8.0284e-02,  8.2945e-03, -2.3384e-01,  4.5981e-02,\n",
       "         4.4365e-01,  8.8301e-02,  1.6314e-01, -7.8315e-02, -7.8813e-01,\n",
       "        -4.8284e-01,  5.8152e-02, -2.1548e-01,  2.3689e-01, -2.1022e-01,\n",
       "         2.0642e-01,  1.3679e-01,  2.5004e-01, -4.8862e-01,  2.4923e-01,\n",
       "         3.5751e-02,  3.5269e-01,  1.1086e-01,  5.3882e-02, -2.9488e-01,\n",
       "        -1.2778e-01,  9.9896e-02, -6.7850e-02,  1.5034e-02, -3.6234e-01,\n",
       "         2.6669e-02,  3.1753e-01, -3.2486e-02, -3.6379e-01, -2.7785e-01,\n",
       "         2.1106e-02,  5.9740e-02, -4.1125e-02,  5.0538e-01,  2.1840e-01,\n",
       "        -2.0883e-01, -3.2456e-01, -3.3017e-01, -9.5862e-02,  5.3931e-01,\n",
       "        -3.8356e-01, -2.9713e-02, -4.6241e-02, -5.0974e-01, -5.2161e-02,\n",
       "        -1.7874e-01,  1.5765e-02, -2.9007e-01,  6.8518e-02,  2.8251e-01,\n",
       "         2.0609e-02, -1.2714e-01,  1.9768e-01,  1.8248e-01,  1.2503e-01,\n",
       "         1.0684e-01,  4.1126e-02,  3.7982e-01, -5.9108e-02,  9.4127e-02,\n",
       "        -3.5327e-01, -6.1653e-02, -6.0000e-01,  4.1299e-02, -6.8123e-02,\n",
       "         3.3920e-01,  2.2421e-01, -4.6697e-01, -8.1151e-02, -3.7642e-01,\n",
       "         1.1570e-01, -2.3701e-01, -3.3112e-01, -1.0839e-01, -1.6000e-01,\n",
       "         1.2492e-01, -5.4287e-03,  6.7560e-03, -3.1358e-01,  8.3053e-02,\n",
       "        -6.3205e-02, -1.0864e-01, -1.3503e-02,  1.7719e-02,  9.1726e-02,\n",
       "         2.4166e-01,  2.7090e-01, -5.0288e-02,  1.8881e-01, -1.7890e-01,\n",
       "         4.5303e-01,  6.4566e-02,  8.1810e-02, -1.5411e-01, -1.8241e-01,\n",
       "        -2.3864e-01,  6.7979e-01, -2.2696e-02, -1.7874e-01, -1.0943e-01,\n",
       "        -1.5450e-01, -2.1170e-02,  8.6160e-02, -2.5742e-01, -1.6707e-01],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentenca[1])\n",
    "sentenca[1].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00344a37-68d2-4d76-ae26-64cbed230152",
   "metadata": {},
   "source": [
    "Vamos ver agora se as tokens \"carro\" e \"veículo\" tem uma representação parecida. Ou seja, se seus embeddings são similares.\n",
    "Para isso, precisamos de alguma medida que meça a similaridade entre dois vetores.\n",
    "Existem diversas forma de fazer isso. Uma das mais comuns é a similaridade de cossenos. Ela consiste simplesmente em medir o\n",
    "ângulo entre os vetores, não se importando com sua magnitude. Dois vetores parecidos vão ter um ângulo entre eles de quase zero.\n",
    "Podemos usar o produto interno para facilmente calcular o cosseno entre os vetores.\n",
    "Se o cosseno entre os vetores der próximo à 1, eles são similares. Se der próximo à 0 ele são distintos.\n",
    "Se der próximos à -1 eles tem significado oposto (e.g. \"bom\" e \"mau\").\n",
    "\n",
    "A fórmula que vamos usar é simples:\n",
    "$$\n",
    "cos(\\theta) = \\frac{\\langle \\mathbf{u},\\mathbf{v} \\rangle}{||\\mathbf{u}|| \\ || \\mathbf{v} ||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a98bc2-d9a6-41e9-a7f4-3a2501cafaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6452633282295562\n",
      "0.5173644488701212\n",
      "0.2285137382744788\n",
      "0.12916372897649708\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similirity_cos(word1, word2):\n",
    "    s1 = Sentence(word1)\n",
    "    s2 = Sentence(word2)\n",
    "    fasttext.embed(s1)\n",
    "    fasttext.embed(s2)\n",
    "    inner_uv = np.inner(s1[0].embedding.tolist(),s2[0].embedding.tolist())\n",
    "    norm_uv = np.linalg.norm(s1[0].embedding.tolist())*np.linalg.norm(s2[0].embedding.tolist())\n",
    "    return inner_uv / norm_uv\n",
    "\n",
    "carro = 'carro'\n",
    "veiculo = 'veículo'\n",
    "porta = 'porta'\n",
    "bom = 'bom'\n",
    "mau = 'mau'\n",
    "otimo = 'ótimo'\n",
    "pessimo = 'péssimo'\n",
    "\n",
    "print(similirity_cos(carro, veiculo))\n",
    "print(similirity_cos(bom, mau))\n",
    "print(similirity_cos(carro, otimo))\n",
    "print(similirity_cos(porta, otimo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e9c40-89a1-4f12-9302-f7f6ec09b041",
   "metadata": {},
   "source": [
    "Implementamos a similaridade de cossenos. Porém, essa função já existe no scikit-learn. Vamos importar e checar com\n",
    "nossa implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e041f07-6b16-425c-a5ac-a9618a09df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64526333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sk_similirity_cos(word1, word2):\n",
    "    s1 = Sentence(word1)\n",
    "    s2 = Sentence(word2)\n",
    "    fasttext.embed(s1)\n",
    "    fasttext.embed(s2)\n",
    "    return cosine_similarity([s1[0].embedding.tolist()], [s2[0].embedding.tolist()])\n",
    "\n",
    "sk_similirity_cos(carro, veiculo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86551e0b-4645-42b8-a6e2-34d70ab21c71",
   "metadata": {},
   "source": [
    "Caso queira remover um embedding já aplicado, basta usar o método `clear_embeddings()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1458e66-4c2a-4e93-a7d1-79909aa36c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenca.clear_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c9c3e-5f76-4158-8690-bd9a22eb3536",
   "metadata": {},
   "source": [
    "### Recuperando palavras de embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c19e74-1d3d-4817-aef1-8882d2d13dc4",
   "metadata": {},
   "source": [
    "E se quisermos o caminho inverso? Ou seja, a partir de um embedding chegar em uma palavra?\n",
    "Isso pode ser feito, mas precisamos de um dicionário de palavras. Esse dicionário de palavras será representado\n",
    "por uma nuvem de pontos no espaço vetorial. Assim, para um vetor qualquer, podemos\n",
    "buscar qual outro vetor no nosso vocabulário que mais se aproxima dele.\n",
    "\n",
    "Vamos criar uma função que faz o embedding de todo o vocabulário e retorna esse vocabulário. Note que\n",
    "essa função vai retornar um objeto do tipo `Sentence` com o atributo de `embedding` já preenchido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae1a1af-9d02-491c-a85c-231f714d4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_pt_vocab(embedding):\n",
    "    dataset = datasets.UD_PORTUGUESE()\n",
    "    vocab_list = dataset.make_vocab_dictionary().get_items()\n",
    "    vocab = Sentence(' '.join(vocab_list))\n",
    "    embedding.embed(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2f82d-4e1d-491e-8756-368cbac57421",
   "metadata": {},
   "source": [
    "Vamos testar a função olhando o resultado de uma das palavras (tokens) do nosso vocabulário. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5926af5d-454d-4b1c-aaff-347551c6112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 10:57:38,637 Reading data from /home/davibarreira/.flair/datasets/ud_portuguese\n",
      "2022-10-18 10:57:38,637 Train: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-train.conllu\n",
      "2022-10-18 10:57:38,638 Dev: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-dev.conllu\n",
      "2022-10-18 10:57:38,639 Test: /home/davibarreira/.flair/datasets/ud_portuguese/pt_bosque-ud-test.conllu\n",
      "Token[6]: \"o\" tensor([ 2.7388e-01,  1.1287e-01, -9.0481e-02, -1.1496e-01,  1.1164e-01,\n",
      "         3.9364e-02, -5.3077e-02,  1.2918e-01, -3.0792e-02, -1.0750e-01,\n",
      "        -1.6031e-02, -2.5077e-02, -7.7410e-02,  8.9467e-02, -2.1526e-01,\n",
      "        -1.2137e-01,  8.9173e-02, -1.8762e-01, -1.2617e-02, -9.3670e-02,\n",
      "        -1.2754e-01, -3.3130e-01,  3.6463e-02, -4.7890e-02,  4.7475e-02,\n",
      "        -1.6050e-01, -2.0617e-01,  1.3285e-02,  3.3718e-03,  2.1074e-02,\n",
      "         3.9722e-01, -2.9204e-02, -1.0872e-01, -8.7509e-03, -1.8711e-01,\n",
      "        -1.5177e-01,  3.8992e-02,  3.0053e-02, -7.5466e-02,  1.8972e-01,\n",
      "         9.5374e-02,  8.3996e-02,  6.1267e-02,  6.0032e-02, -5.1243e-02,\n",
      "        -5.2345e-03,  1.0803e-01,  1.7155e-01,  9.1747e-02, -4.7930e-02,\n",
      "        -9.1886e-02,  2.8991e-02,  1.8948e-01, -4.3372e-02, -7.9234e-02,\n",
      "        -1.7530e-01,  1.2217e-01,  1.1195e-01,  2.9282e-02, -3.3850e-02,\n",
      "         4.1536e-02, -1.5603e-01,  3.0681e-02, -1.1030e-01, -6.5739e-02,\n",
      "         1.9232e-02, -4.8071e-02, -3.6749e-02, -8.7108e-02, -4.9993e-02,\n",
      "        -7.6168e-02,  3.5094e-02, -1.7885e-02, -2.2317e-01, -3.5243e-02,\n",
      "        -1.8885e-01, -8.5777e-02, -1.2359e-01,  6.1548e-02,  2.4146e-01,\n",
      "        -1.2403e-01,  4.2002e-02,  6.3104e-02, -1.3010e-01, -1.0023e-02,\n",
      "         8.8140e-03,  2.9701e-01, -3.0200e-02,  7.3541e-02,  1.1278e-02,\n",
      "         5.3558e-02, -1.6980e-01, -6.5448e-02, -1.9631e-02,  9.7226e-02,\n",
      "        -2.0808e-01, -6.8677e-02,  8.7391e-02, -7.9956e-02, -9.2281e-02,\n",
      "        -6.7199e-02, -8.0677e-03,  5.0649e-02, -2.2739e-01,  3.3343e-02,\n",
      "        -7.6248e-03, -8.5515e-02, -1.9198e-01,  6.6061e-02, -2.5240e-01,\n",
      "        -7.3555e-02,  2.2959e-01, -3.1300e-02,  2.5391e-02, -2.3407e-01,\n",
      "         1.6791e-01,  2.6421e-02,  8.7497e-02,  1.7753e-02, -2.3025e-02,\n",
      "         5.0735e-02, -5.0933e-02, -1.0146e-02, -6.1014e-02, -2.4289e-02,\n",
      "         7.4864e-02, -1.9503e-01,  9.0427e-02, -1.2701e-01,  4.3742e-02,\n",
      "         4.2221e-02, -5.4097e-02,  9.8396e-02,  2.4758e-02, -1.1601e-02,\n",
      "         4.3430e-02,  2.3380e-02,  2.2909e-01,  2.6701e-02, -5.3898e-02,\n",
      "         5.1685e-02, -5.3249e-04,  1.5025e-01, -1.4933e-01,  2.5019e-01,\n",
      "         1.0839e-02, -4.6172e-02, -1.2937e-01, -1.0944e-01,  8.7821e-02,\n",
      "         6.7018e-02, -8.4090e-02, -4.4625e-01,  2.8492e-02,  2.9175e-02,\n",
      "         8.1417e-02,  6.2188e-02, -2.9222e-01,  6.6813e-02,  3.6123e-03,\n",
      "         2.0620e-02,  1.6462e-01,  2.2310e-01,  9.5378e-02, -1.5649e-01,\n",
      "         3.7755e-02,  4.3869e-02, -8.7248e-02,  6.5152e-02,  9.7387e-02,\n",
      "         1.9668e-03,  1.0818e-01, -3.4086e-02, -1.5168e-01, -2.0768e-01,\n",
      "        -1.6697e-01, -3.4135e-02, -3.2365e-02,  1.4630e-01, -4.0056e-03,\n",
      "         8.3258e-02, -4.7585e-02, -1.8155e-01,  5.5088e-02,  2.2571e-01,\n",
      "         1.8136e-01,  5.2192e-02, -1.1697e-01,  1.1289e-01,  2.4359e-02,\n",
      "        -1.7906e-01,  8.9905e-02, -3.5223e-02, -1.6566e-01,  9.2322e-02,\n",
      "         2.1348e-01, -2.4276e-01, -5.8528e-02,  9.0273e-02, -6.4855e-02,\n",
      "        -2.0065e-01,  1.3543e-01, -5.4502e-02,  7.2364e-02, -8.7802e-02,\n",
      "         4.0301e-01,  6.7565e-03, -1.9266e-01, -1.4126e-01, -6.0843e-02,\n",
      "        -1.8571e-01,  5.5142e-02, -5.5698e-02,  6.7832e-02,  1.3136e-01,\n",
      "         3.0695e-02,  4.3377e-02, -3.3958e-01,  1.0306e-01,  1.9781e-01,\n",
      "        -7.9297e-02, -5.3879e-02,  1.8205e-02, -1.7651e-01, -8.5641e-02,\n",
      "        -2.5143e-01, -1.4166e-01,  5.3386e-02, -1.6136e-01, -1.8480e-01,\n",
      "         1.0889e-01,  6.3974e-02, -1.6384e-01, -3.0337e-01,  1.3286e-01,\n",
      "        -1.4050e-01, -1.9278e-03, -2.6509e-01, -6.1120e-02,  4.9653e-02,\n",
      "        -1.2584e-02, -1.8807e-01,  6.3102e-03,  3.2251e-01,  7.5652e-03,\n",
      "        -8.9616e-02,  1.1768e-01,  5.9212e-02, -1.4969e-01,  2.1412e-01,\n",
      "        -2.0440e-01, -4.7088e-06, -1.6058e-02, -3.5501e-02,  4.6736e-02,\n",
      "        -1.7489e-01,  2.9655e-01, -1.6929e-01,  3.5215e-02,  1.7453e-01,\n",
      "         3.4799e-02, -4.2587e-02, -1.2533e-01,  2.1024e-01, -2.1680e-01,\n",
      "         3.1041e-02, -3.2641e-02,  6.9289e-02,  1.1365e-01, -1.0633e-01,\n",
      "         2.2553e-01,  5.4853e-02,  4.8188e-02, -1.4703e-01,  9.0899e-02,\n",
      "         5.8069e-02,  1.0415e-01,  1.1069e-01, -7.0235e-02, -1.1329e-01,\n",
      "         1.6130e-01,  1.7306e-01, -1.9017e-01, -1.2030e-01,  3.7571e-02,\n",
      "         9.5399e-02, -1.2405e-01,  1.9312e-01, -1.4948e-01,  7.1201e-02,\n",
      "        -1.4507e-01,  1.0513e-01,  1.4128e-02,  2.4941e-02, -2.4757e-01,\n",
      "         3.9026e-02, -2.6660e-02,  3.2811e-02, -2.5585e-02, -2.1247e-01],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vocab = get_embedded_pt_vocab(fasttext) \n",
    "print(vocab[6], vocab[6].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d40d3-bdd1-4c7e-92bd-30927586f11d",
   "metadata": {},
   "source": [
    "Considere agora a palavra \"bom\". Vamos criar o seu vetor. A pergunta é, qual a palavra no nosso vocabulário\n",
    "que é o oposto de \"bom\"? Para isso, obtemos o embedding de \"bom\" e em seguida o multiplicamos por -1. Por fim,\n",
    "buscamos no dicionário a palavra que mais se aproxima desse embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4fcce08-bdce-4605-89d3-6a3688233379",
   "metadata": {},
   "outputs": [],
   "source": [
    "bom = Sentence('bom')\n",
    "fasttext.embed(bom)\n",
    "\n",
    "emb = -1*np.array(bom[0].embedding.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1386f07-5ccd-4ffe-a0fa-256890e8058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000007"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([emb],[list(emb)])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54083477-d46d-455d-9959-f24513b966ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `emb` é o embedding\n",
    "\n",
    "def find_closest_matching_word(emb, vocab):\n",
    "    max_match = -1\n",
    "    for word in vocab:\n",
    "        match = cosine_similarity([emb], [word.embedding.tolist()])[0][0]\n",
    "        if match > max_match:\n",
    "            max_match = match\n",
    "            closest_matching_word = word.text\n",
    "    return closest_matching_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b5aed3a-0b03-4e7d-9113-830c637c501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sade'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_matching_word(emb, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a0f4f-f9f2-4a3c-b473-35d92bcca100",
   "metadata": {},
   "source": [
    "... O resultado não deu bem o que esperávamos. \n",
    "\n",
    "Isso quer dizer que nosso embedding não está performando tão bem quanto desejado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036d66b-81bd-451f-b739-a36701c0818a",
   "metadata": {},
   "source": [
    "## Flair Embeddings\n",
    "\n",
    "Como mostramos no exemplo passado, o embedding original \"FastText\" não se mostrou muito preciso.\n",
    "No \"Flair\" temos um embedding especial chamado de \"Flair Embedding\".\n",
    "Esse embedding é bastante poderoso, e utiliza o contexto na hora de gerar o embedding. Ou seja,\n",
    "ele não considera a palavra isoladamente. Isso é fundamental, pois sabemos que a mesma palavra\n",
    "pode ter significados totalmente diferentes dado um contexto. Por exemplo, \"manga\" a fruta e \"manga\" de uma camisa.\n",
    "Se usarmos um único embedding para representar a palavra \"manga\", não faz sentido falar da proximidade da palavra\n",
    "\"manga\" e \"caju\", pois \"manga\" também é a parte de uma roupa, que nada tem a ver com uma fruta.\n",
    "\n",
    "A primeira coisa a se entender dos Flair Embeddings é que existem dois tipos, o \"forward\" e o \"backward\". O \"forward\"\n",
    "considera como contexto somente o que vem antes da token, enquanto que o \"backward\" considera\n",
    "como contexto somente o que vem depois.\n",
    "Assim, nas senteças \"manga da camisa\" e \"manga boa de comer\", o embedding \"forward\" da token \"manga\" será igual para as duas sentenças,\n",
    "mas o \"backward\" será diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "149b6173-0bb6-4afc-b62b-84148c9aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "flair_embedding_forward = FlairEmbeddings('pt-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('pt-backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c346267-5d68-4ca5-ae3b-c8b321f556e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "s1 = Sentence(\"manga da camisa\")\n",
    "s2 = Sentence(\"manga boa de comer\")\n",
    "\n",
    "flair_embedding_forward.embed(s1)\n",
    "flair_embedding_forward.embed(s2)\n",
    "\n",
    "print(s1[0].embedding.tolist() == s2[0].embedding.tolist())\n",
    "\n",
    "flair_embedding_backward.embed(s1)\n",
    "flair_embedding_backward.embed(s2)\n",
    "print(s1[0].embedding.tolist() == s2[0].embedding.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7da24ff-6e2a-4dc6-a339-6e78e5d8d0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86898918]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = Sentence('ele é uma pessoa boa')\n",
    "s2 = Sentence('ele é uma pessoa excelente')\n",
    "flair_embedding_forward.embed(s1)\n",
    "flair_embedding_forward.embed(s2)\n",
    "\n",
    "cosine_similarity([s1[-1].embedding.tolist()],[s2[-1].embedding.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c318d-9cc1-4208-b05c-1c9044b0a452",
   "metadata": {},
   "source": [
    "Outro aspecto interessante do Flair é que ele é capaz de lidar com palavras fora do vocabulário. Veja, caso escrevamos\n",
    "uma palavra errada, ele ainda é capaz de fazer um embedding, inclusive retornando uma similaridade boa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95d08172-a37b-430a-be3e-430e4c43f267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76655116]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = Sentence('ele é uma pessoa boa')\n",
    "s2 = Sentence('ele é uma pessoa boaa')\n",
    "flair_embedding_forward.embed(s1)\n",
    "flair_embedding_forward.embed(s2)\n",
    "\n",
    "cosine_similarity([s1[-1].embedding.tolist()],[s2[-1].embedding.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cf1de-55f0-4e5a-8d18-48078d48cf08",
   "metadata": {},
   "source": [
    "## PooledFlairEmbeddings\n",
    "\n",
    "Esse outro embedding mantém um contexto global das tokens, de forma que o seu embedding vai mudando a medida que mais\n",
    "ocorrências da mesma palavra vão acontecendo. Assim, a mesma palavra com mesmo contexto poderá ter um novo embedding, caso\n",
    "já tenha ocorrido na sentença em algum outro momento.\n",
    "\n",
    "O uso desse embedding é \"identico\" ao FlairEmbedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f5e270-e810-4145-89ec-049be197f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import PooledFlairEmbeddings\n",
    "\n",
    "pooled_forward  = PooledFlairEmbeddings('pt-forward')\n",
    "pooled_backward = PooledFlairEmbeddings('pt-backward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a760f-d84a-4031-8193-777b29e83dc8",
   "metadata": {},
   "source": [
    "## Stacked embeddings\n",
    "\n",
    "Mas, existem palavras que o contexto envolve tanto o que vem antes como o que vem depois... Portanto, precisamos\n",
    "de alguma forma combinar os embeddings \"forward\" e \"backward\". Para isso, usamos o Stacked Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71e3d180-e17d-470c-a3b2-5eac0ed63147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import StackedEmbeddings\n",
    "fasttext = WordEmbeddings('pt')\n",
    "flair_fw = FlairEmbeddings('pt-forward')\n",
    "flair_bw = FlairEmbeddings('pt-backward')\n",
    "combined_embeddings_list = [fasttext, flair_fw, flair_bw]\n",
    "\n",
    "stack = StackedEmbeddings(combined_embeddings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe329a2-efe2-4f8a-ae60-38a7646ea7d7",
   "metadata": {},
   "source": [
    "Novamente, esse embedding funciona da mesma forma que os demais. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc74098-247f-4153-9c85-5a29706360d8",
   "metadata": {},
   "source": [
    "## Transformer Embedding\n",
    "\n",
    "Além de todos esses, o Flair possui ainda mais outras opções de embedding. Uma especialmente útil são os embeddings usando Transformers.\n",
    "Podemos tanto usar modelos pré-treinados do Hugging Face, como podemos usar modelos próprios, contanto que sua arquitetura tenha suporte no Flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a6a90e-bfe3-43f3-b267-e4d8de4e7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from legalnlp.get_premodel import *\n",
    "# get_premodel('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddd5b73a-8fc2-4b9a-9133-6d2df796231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7179cf7c-e0d7-469b-9622-261b2beaab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "sentenca = Sentence('O juiz julgou procedente.')\n",
    "embeddings = TransformerWordEmbeddings('./models/BERTikal/', layers='-1', layer_mean=False)\n",
    "embeddings.embed(sentenca)\n",
    "print(sentenca[0].embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6aa25e21-e37b-4c3b-951a-3b48a3e95b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenca.clear_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527c871-c20c-4add-ad0e-18a761901e17",
   "metadata": {},
   "source": [
    "## Document Embedding\n",
    "\n",
    "Mostramos como representar palavras como vetores. O mesmo pode ser feito para textos como um todo. Ou seja,\n",
    "podemos representar um documento como um vetor, e usar a semelhança de cossenos para avaliar quão parecidos são dois documentos.\n",
    "Esse tipo de ideia nos permite fazer coisas como clusterizar documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75c4bd7a-b9b9-46f5-9e42-f80f4ca060ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768]) tensor([-3.6046e-02,  3.8304e-01, -5.0739e-01,  4.7760e-01,  1.0422e-01,\n",
      "         9.2792e-01, -3.1415e-01, -2.3043e-01, -3.1315e-01,  5.7292e-01,\n",
      "         1.5351e-01,  6.1514e-01,  6.3847e-02, -5.4561e-01,  5.1991e-01,\n",
      "         3.6589e-01,  9.9137e-01, -6.0815e-02,  6.9953e-02,  3.7254e-01,\n",
      "        -1.2381e+00,  9.2895e-01, -6.8616e-02, -6.5012e-02,  7.1273e-01,\n",
      "         1.4171e-01,  1.3703e-01,  1.4975e-01,  3.4588e-01, -9.2155e-02,\n",
      "         1.8884e-01,  3.2722e-01,  4.6638e-01,  8.0505e-02,  6.7849e-02,\n",
      "         4.6737e-01,  4.4417e-01,  1.0906e+00,  1.1045e-01,  1.9594e-01,\n",
      "         2.7253e-01,  3.1203e-01,  3.8897e-01, -7.7073e-01, -2.0878e-01,\n",
      "        -3.4655e-01,  3.2428e-01,  5.9096e-01, -1.1603e-01,  7.6025e-02,\n",
      "        -4.2636e-01,  2.1179e-01, -5.4417e-01, -3.7778e-01, -4.8404e-01,\n",
      "        -2.2989e-01,  2.3265e-01, -2.9809e-01, -1.8232e-01, -3.1540e-01,\n",
      "         5.6045e-01, -3.9891e-01, -3.9693e-02, -7.5340e-01, -1.2653e-01,\n",
      "        -8.0830e-01, -1.5234e-01, -1.9319e-01,  6.0476e-01,  1.8731e-01,\n",
      "         2.1413e-01, -4.9130e-01,  5.3557e-01,  5.5092e-01, -7.0239e-01,\n",
      "         6.9585e-01,  9.1949e-01, -4.3789e-01, -1.3476e+00,  2.2217e-01,\n",
      "         2.7974e-01,  5.6744e-01,  6.3733e-01,  4.5154e-01,  2.6825e-01,\n",
      "         1.7330e-01,  1.7417e-01,  6.2721e-02,  5.9384e-01,  1.6546e-01,\n",
      "         1.2932e+00,  6.2001e-02,  1.8712e-01,  8.7411e-02,  5.4315e-01,\n",
      "         7.4542e-01,  2.9025e-01, -1.1429e-01,  7.8058e-02, -4.6180e-01,\n",
      "         6.8644e-01,  1.0663e+00,  8.1183e-01, -8.1769e-02, -2.2541e-01,\n",
      "        -1.3716e-02, -2.6935e-01, -2.8866e-01, -4.8057e-01,  4.3274e-01,\n",
      "        -4.3119e-01,  1.0014e-01, -6.6593e-01,  4.7898e-01, -2.5135e-01,\n",
      "         1.3202e-01,  2.1053e-01,  4.7025e-01,  1.2131e-02, -2.5877e-01,\n",
      "         4.0327e-01,  3.2957e-01, -9.6501e-02, -1.4175e+00, -2.1114e-01,\n",
      "         5.1012e-01, -6.0255e-01, -7.2513e-01,  5.6064e-02, -5.7768e-01,\n",
      "        -7.4439e-02, -4.0445e-01, -3.3230e-01,  8.9843e-01,  4.0579e-01,\n",
      "         4.1313e-01,  5.3029e-01,  5.3083e-01, -1.9522e-01,  8.6903e-01,\n",
      "         1.1863e+00,  7.1868e-01,  2.5589e-01,  9.0835e-01,  2.8288e-01,\n",
      "        -7.9874e-01, -1.1407e+00,  4.8616e-01,  2.5613e-01,  2.0931e-01,\n",
      "         4.5780e-01,  6.0690e-02, -5.2728e-02,  1.7994e-01, -1.3044e-01,\n",
      "         9.7375e-01, -2.3974e-01, -1.7830e-01, -1.2730e-01, -3.9564e-01,\n",
      "        -1.5929e-01,  7.4110e-02, -2.9957e-01, -1.0196e+00, -6.6407e-01,\n",
      "        -9.2341e-01,  8.8072e-01,  2.6812e-02, -3.6260e-01,  2.3586e-01,\n",
      "         2.5307e-01,  2.6155e-01, -7.1614e-01,  4.9416e-01,  2.0874e-01,\n",
      "        -5.1310e-01,  3.4630e-01, -1.3470e-01,  9.1884e-02,  4.1325e-01,\n",
      "         4.6725e-01,  1.2157e-01, -1.3658e-01, -7.8851e-01,  1.6766e-02,\n",
      "         7.5779e-01,  3.5964e-01, -5.1677e-01, -4.5871e-02,  6.9465e-01,\n",
      "         7.6818e-03,  2.3946e-01,  3.6330e-01, -5.1952e-01, -4.5474e-02,\n",
      "        -4.1157e-01,  2.2889e-01, -7.3014e-02, -5.6152e-01, -7.3378e-01,\n",
      "        -8.9185e-01,  2.9585e-01, -3.6816e-01,  1.6313e-01, -4.2696e-02,\n",
      "        -1.0315e-01, -3.9665e-01, -6.4260e-01, -4.6828e-01,  4.4460e-01,\n",
      "         7.2385e-02,  2.8454e-02,  2.8383e-01, -3.8697e-01, -4.9414e-01,\n",
      "         1.2825e-01, -2.8154e-01, -4.2073e-01,  9.4454e-02,  1.2272e+00,\n",
      "         3.1366e-01,  4.2327e-01,  1.1914e-01, -1.6955e-01,  3.6989e-03,\n",
      "        -3.5753e-02, -8.1547e-02,  6.1762e-01,  3.4902e-01,  4.5957e-01,\n",
      "        -9.8585e-01,  7.3299e-02, -3.5918e-01, -1.8740e-01, -2.7427e+00,\n",
      "        -6.0464e-01,  1.6962e-01, -4.9813e-01, -3.4422e-01, -3.2704e-01,\n",
      "         2.3403e-01,  6.0057e-01,  2.3148e-01, -7.2767e-01, -3.4222e-01,\n",
      "         7.9312e-01,  2.2075e-01,  1.5420e-01, -2.7025e-01,  4.1152e-03,\n",
      "        -4.8984e-01,  9.4065e-02,  9.3773e-01, -3.5869e-02,  1.4585e-01,\n",
      "        -6.3868e-01,  2.1973e-02, -1.4071e-01,  1.1206e-01, -9.5482e-01,\n",
      "         4.3898e-01, -2.2278e-01, -2.9908e-02,  1.5978e-01,  6.6714e-01,\n",
      "        -3.6673e-01, -1.9014e-01, -5.0706e-01, -4.6178e-01, -4.4566e-01,\n",
      "        -8.1586e-01,  8.2900e-01,  3.5337e-01,  3.1006e-01, -9.0112e-01,\n",
      "         5.1033e-01,  8.1992e-01,  2.7464e-01,  1.2901e-01, -2.5749e-01,\n",
      "        -3.8493e-02,  1.6186e-01, -5.5685e-01, -3.5769e-01,  1.2860e-01,\n",
      "         2.7445e-01,  7.0112e-01, -1.5907e-01,  3.7035e-01,  1.1777e-01,\n",
      "         8.5865e-01, -5.8106e-01, -6.1554e-01,  4.7678e-01,  3.9235e-02,\n",
      "        -1.0382e+00,  5.0281e-01,  6.5416e-01,  9.9883e-01, -5.2707e-01,\n",
      "         1.4520e-01,  7.1789e-01,  4.9525e-01, -8.5990e-02,  9.6009e-02,\n",
      "         3.3704e-01, -1.6038e-01, -1.2096e+00,  1.1523e+00,  4.9698e-01,\n",
      "         5.6311e-01, -4.4327e-01,  3.2547e-01,  1.9118e-01,  3.2387e-01,\n",
      "        -5.0604e-01,  1.5545e-01, -8.7318e-01, -3.4897e-01,  1.0653e-01,\n",
      "         9.4479e-01,  1.4481e-01, -4.1225e-01, -3.9190e-01, -7.8603e-02,\n",
      "         1.3234e-01,  2.0255e-01, -3.6736e-01,  1.3627e-01,  9.6199e-03,\n",
      "        -1.1533e+00,  9.1365e-01,  7.4563e-01,  3.1735e-01, -5.1641e-02,\n",
      "        -5.2580e-02, -9.9019e-02,  9.6347e-03, -4.3364e-01, -2.7574e-01,\n",
      "         8.5210e-02, -1.0090e+00,  1.2699e+00, -6.1113e-01, -4.3916e-02,\n",
      "        -2.1826e-01,  2.3696e-01,  1.0847e+00,  2.5069e-01, -3.8855e-01,\n",
      "        -4.9710e-01,  1.1707e-01,  5.8733e-01,  5.1287e-01, -7.9546e-01,\n",
      "         1.3161e-01, -8.1301e-01, -3.2871e-01, -1.7346e-01, -2.3117e-01,\n",
      "        -3.1117e-01, -9.3657e-02, -5.9536e-01, -2.2620e-01, -5.6226e-01,\n",
      "         2.1967e-01, -7.9493e-01, -3.5753e-01, -1.1778e-01,  5.9621e-03,\n",
      "         3.0884e-01,  6.1482e-01, -1.1473e+00, -1.5712e+00,  6.3819e-01,\n",
      "        -1.8784e-01, -2.5752e-01,  2.4963e-03,  9.7477e-01, -5.8277e-02,\n",
      "         1.8548e-01, -3.4707e-01, -2.2059e-01,  1.2401e-01, -7.5093e-02,\n",
      "        -2.0170e-01, -1.5256e-01,  1.5536e-01,  3.3918e-01, -2.3353e-01,\n",
      "         2.7121e-02,  5.2313e-01, -5.5521e-01,  7.8488e-01,  8.8557e-03,\n",
      "        -6.4630e-02,  4.4412e-01,  3.1118e-02, -4.7307e-01, -3.8761e-01,\n",
      "         2.9511e-01, -7.0312e-01,  1.1978e+00,  2.4445e-01, -8.2228e-01,\n",
      "         3.1194e-02, -2.1925e-01, -9.1309e-01, -2.1296e-01,  3.6668e-01,\n",
      "        -1.5006e-02,  2.4337e-02,  1.7524e-01,  4.6456e-01,  5.7442e-01,\n",
      "         7.1608e-01, -5.4960e-01,  1.5530e-01,  1.5527e-01,  7.0306e-01,\n",
      "         7.3011e-01,  1.9152e-01, -3.7428e-01, -1.4444e+00, -4.4916e-02,\n",
      "        -2.3274e-01,  3.4993e-01, -2.1997e-01, -1.5024e-01, -2.0977e-01,\n",
      "        -4.8223e-01,  7.2326e-01,  2.0798e-01,  4.3703e-01,  5.3551e-01,\n",
      "        -3.5636e-01, -4.6829e-02, -9.9756e-02, -5.7163e-02, -5.6683e-01,\n",
      "         3.9098e-01, -1.9320e-01, -1.0233e+00, -3.1331e-03,  3.5050e-01,\n",
      "        -1.7849e-01,  2.3880e-02,  8.4633e-02, -3.9462e-02, -7.6027e-02,\n",
      "         2.3511e-01, -1.4663e-01,  5.2474e-01,  7.4795e-01, -1.0129e+00,\n",
      "        -3.1585e-01, -4.1764e-01,  1.0146e+00,  3.1939e-01, -3.0370e-01,\n",
      "         1.3727e-01, -2.7496e-01,  1.4275e-01, -6.4147e-01,  2.1271e-02,\n",
      "        -2.2711e-01, -4.6393e-01,  3.8644e-01,  3.1984e-01,  1.1554e+00,\n",
      "        -1.0058e+00,  2.9525e-02, -5.9780e-01,  5.3490e-02, -1.0100e+00,\n",
      "         5.1794e-01, -5.1147e-01,  2.1271e-01,  9.6211e-02,  2.4798e-02,\n",
      "         1.7383e-01,  6.4354e-01,  6.1652e-01, -3.9303e-01, -7.3703e-01,\n",
      "         2.1271e-01, -3.1368e-01,  7.3664e-01, -3.5546e-02,  1.8641e-01,\n",
      "         3.5356e-01,  1.4730e-01,  2.2704e-01, -9.0328e-02, -6.9851e-01,\n",
      "         3.6636e-01, -2.3823e-01,  3.3730e-02, -5.3616e-01, -5.2435e-01,\n",
      "         1.5726e-01,  4.1816e-01,  7.5088e-01,  8.5805e-01, -1.6800e-01,\n",
      "         2.0683e-01, -4.5533e-01, -1.9432e-01, -1.6314e-01, -1.3696e-01,\n",
      "        -5.8619e-01, -6.9809e-01,  6.7229e-01, -5.5349e-01, -4.7716e-01,\n",
      "        -3.3663e-01,  7.4994e-01,  3.6275e-01,  1.9372e-01, -3.6047e-01,\n",
      "        -1.3841e+00, -2.7186e-01,  6.3111e-01,  5.2085e-01, -4.1757e-01,\n",
      "         1.7624e-02, -5.8014e-01, -5.0060e-01, -2.8261e-01, -2.1213e-01,\n",
      "        -6.4805e-01, -1.7941e-01, -3.2797e-01,  2.7704e-01,  4.7374e-02,\n",
      "        -7.4852e-03, -1.3340e-01,  5.8083e-01,  8.6688e-01,  1.0350e+00,\n",
      "        -5.5791e-01,  4.2514e-01, -1.0938e+00, -1.1486e+00,  7.6252e-01,\n",
      "        -5.0863e-01, -8.0020e-02, -5.0507e-01, -6.6901e-01, -4.3615e-02,\n",
      "         4.8813e-01,  7.8818e-01, -5.7776e-01, -2.1475e-02,  7.9347e-02,\n",
      "         3.4743e-01,  1.9023e-01, -6.1198e-01,  6.3714e-01,  4.0311e-01,\n",
      "        -4.0743e-01,  2.4569e-01, -6.3440e-01,  9.2233e-01,  8.9064e-01,\n",
      "         3.2255e-01, -1.2563e+00,  2.9530e-01,  8.8368e-01, -2.8915e-01,\n",
      "         1.8911e-01, -2.3849e-01, -7.4604e-03,  2.9336e-01, -3.0313e-01,\n",
      "         6.3976e-01, -2.6920e-01, -7.4415e-02, -3.7097e-01,  3.3229e-02,\n",
      "        -1.0952e+00, -1.5331e-01,  3.1872e-01, -3.0347e-03, -7.2450e-01,\n",
      "        -2.7666e-01,  9.9107e-02,  5.0948e-01,  3.2937e-01, -4.1814e-01,\n",
      "        -8.9655e-01,  8.7065e-01, -2.3085e-01,  3.4473e-02,  5.4430e-01,\n",
      "        -9.8154e-02,  1.1855e-01, -4.1800e-01, -7.9123e-01,  1.9990e-01,\n",
      "        -4.3295e-03,  2.1549e-01,  4.0427e-01,  8.3572e-01,  3.9700e-01,\n",
      "        -6.2861e-01,  4.0163e-01, -2.3444e-01, -8.9345e-01, -3.4991e-01,\n",
      "         4.7925e-01, -3.9312e-01,  1.8544e-02,  2.5172e-02, -1.9295e-01,\n",
      "        -5.5649e-02, -2.0155e-02, -7.1593e-01,  3.3793e-01,  2.7591e-01,\n",
      "         3.7470e-02, -1.6500e-01,  2.2729e-01, -8.4097e-01,  5.1794e-01,\n",
      "         7.4894e-02,  5.0418e-01, -3.1730e-01, -2.8446e-01, -5.2381e-01,\n",
      "         2.6455e-02,  5.6018e-02, -4.7776e-01, -4.3700e-02, -3.8683e-01,\n",
      "         1.0548e+00, -2.2985e-01,  6.7082e-02, -4.1679e-01,  1.5711e-02,\n",
      "        -1.1011e-02, -2.4424e-02, -3.5567e-01,  6.6777e-01, -7.1144e-01,\n",
      "         3.1123e-01, -1.0267e-01, -1.8560e-01, -6.4590e-04, -4.5920e-01,\n",
      "        -1.7016e-01, -2.1001e-01,  3.3087e-01, -2.7507e-01, -2.8003e-02,\n",
      "        -1.3781e-01,  3.9265e-02,  8.5396e-02, -3.9412e-02,  3.9254e-01,\n",
      "        -5.8470e-02,  2.4663e-02,  4.4055e-01, -3.7120e-01,  2.1417e-01,\n",
      "         5.5979e-01, -3.4844e-01, -7.5328e-01, -2.0265e-01, -1.1176e-01,\n",
      "        -5.2418e-01, -4.1116e-01,  1.1100e-01, -3.3399e-02,  1.3005e-01,\n",
      "         6.1859e-01, -1.8121e-01, -2.5543e-01,  8.0728e-01,  1.1746e-01,\n",
      "        -1.5729e-01, -9.6062e-02,  3.8748e-01, -3.2104e-01, -2.1887e-01,\n",
      "         3.0437e-01, -1.3832e-01,  4.3881e-01, -4.6885e-01, -8.0662e-01,\n",
      "        -7.1357e-01, -5.0493e-01,  1.0024e+00,  5.4098e-02,  9.1162e-01,\n",
      "        -6.0837e-01,  2.1566e-01,  2.1818e-01,  5.9231e-01,  7.3479e-01,\n",
      "        -3.8351e-01, -4.1007e-01,  7.1630e-01,  6.3126e-01, -2.6112e-01,\n",
      "        -8.5013e-01, -1.9240e-01, -1.0493e+00,  4.0981e-01,  3.3590e-01,\n",
      "         3.6068e-02,  1.0705e+00,  6.9073e-01, -9.4084e-01,  4.8383e-01,\n",
      "        -3.7781e-01, -6.1346e-01, -3.2189e-01,  4.6007e-01,  8.0589e-01,\n",
      "         6.2963e-01, -5.1511e-02,  6.0964e-01, -3.1922e-01,  4.3812e-01,\n",
      "        -4.4116e-01, -3.6798e-01, -1.3001e-02, -7.9464e-02, -1.0333e-01,\n",
      "         2.8360e-01, -1.5726e+00, -5.0637e-01,  1.1354e-01,  3.3361e-01,\n",
      "        -3.8887e-02, -4.4594e-01, -1.8515e-01, -4.9766e-01,  7.0420e-02,\n",
      "        -2.1203e-01, -3.1716e-01, -6.6890e-01,  3.0306e-01, -3.3919e-02,\n",
      "        -1.0107e+00,  7.8300e-01, -3.0352e-02,  5.3751e-01,  6.2360e-01,\n",
      "        -3.3432e-02, -5.4793e-01, -1.9179e-01, -3.8124e-01,  8.7179e-02,\n",
      "         1.0634e+00, -7.8676e-01,  4.1207e-01,  2.9993e-01,  7.8342e-01,\n",
      "        -4.7410e-02,  7.5846e-02, -3.4794e-01, -4.9605e-01,  2.6369e-01,\n",
      "        -5.9161e-01,  1.0324e-01, -4.0949e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "embedding = TransformerDocumentEmbeddings('./models/BERTikal/')\n",
    "\n",
    "sentenca = Sentence('O juiz julgou procedente.')\n",
    "embedding.embed(sentenca)\n",
    "\n",
    "print(sentenca.embedding.size(),sentenca.embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
